## Background  
### class  
  - An object or group of objects with similar properties  
  - In image classification, the category of the target want to predict  
   ex) cars, dogs, cats, palnes, etc  
### instance
  - Specific examples of classes or individual objects of classes
  - In image classfication, a representation of an object or particular object in an image  

## NPID: Unsupervised Feature Learning via Non-Parametric Instance Discrimination [Z. Wu et al., 2018]
### Goal  
  - Capturing the features of data well with unsupervised learning and achieving superior performance in supervised learning tasks
### Motivations  
  - Supervised learning results that discover apparent similarity among semantic categories without being explicitly guided to do so
      - Learning how to distinguish differences between individual instances independently of sementic categories
  - Using unsupervised learning effectively as instance-level discrimination
      - Learning how to discriminate differences of similarities between input data
### Contributions  
  - Solving the problem of the parametric softmax using the non-parametric approach
      - Generalizing predictable for new classes as well
  - Solving high costs of softmax with Noise-Contrastive Estimation and memory bank
      - Changing multi-class classification problems into binary classification problems
      - Performing relatively simply
### Methods  
#### Approach  
  - Learning an embedding function $v=ğ’‡_ğœ½(ğ’™)$ without supervision  
  - Novel unsupervised feature learning approach wwith Instance-level discrimination
      - Treating each image instance as a distinct class of its own
      - Training a classifier to distinguish between individual instance classes
  - $ğ’…_ğœ½ (ğ’™,ğ’š)=â€–ğ’‡_ğœ½ (ğ’™)âˆ’ğ’‡_ğœ½ (ğ’š)â€–$
      - $ğ’‡_ğœ½$: a deep neural network with parameters $ğœ½$  
      - $ğ’™$: mapping image  
      - $ğ’—$: feature vector
    ![image](https://github.com/MINJEONG-L/NPID/assets/82145878/6d62fea2-0af2-4846-b20a-e2cbd1080376)

#### Non-parametric softmax classifier  
  - Parametric classifier
      - Formulating the instance-level classification objective using the softmax criterion
      - $ğ‘·(ğ’Šâ”‚ğ’—)=(ğ’†ğ’™ğ’‘(ğ’˜_ğ’Š^ğ‘» ğ’—))/(âˆ‘2_(ğ’‹=ğŸ)^ğ’â–’ã€–ğ’†ğ’™ğ’‘(ğ’˜_ğ’‹^ğ‘» ğ’—)ã€—)$
          - $ğ’—_ğ’Š$: feature of the $ğ’Š$-th image  
          - $ğ’˜_ğ’‹^ğ‘»$: transpose of the weight vector for class $ğ’‹$  
          - Measuring the probability that the feature vector ğ’— belongs to the $ğ’Š$-th instance  
  - Non-parametric classifier
      - Resolving the issues with the parametric softmax formulation
          - Hiding the explicit comparison between instances through ğ’˜ acting as a class prototype
          - $ğ‘·(ğ’Šâ”‚ğ’—)=(ğ’†ğ’™ğ’‘((ğ’—_ğ’Š^ğ‘» ğ’—)/ğ‰))/(âˆ‘2_(ğ’‹=ğŸ)^ğ’â–’ã€–ğ’†ğ’™ğ’‘((ğ’—_ğ’‹^ğ‘» ğ’—)/ğ‰)ã€—)!$
            - $ğ‰$: temperature parameter
            - Replacing $ğ’˜_ğ’‹^ğ‘» ğ’—$ with $ğ’—_ğ’‹^ğ‘» ğ’—$
          - $ğ‘±(ğœ½)=âˆ’âˆ‘_(ğ’Š=ğŸ)^ğ’â–’ã€–ğ¥ğ¨ğ ğ‘·(ğ’Š|ğ’‡_ğœ½ (ğ’™_ğ’Š ))ã€—$
          - Non-parametric softmax formulation facilitates comparison between instances
    - Memory bank
        - Requiring to cumpute the probability $ğ‘·(ğ’Šâ”‚ğ’—), {ğ’—_ğ’‹}$ for all images
        - Memory Bank $ğ‘½={ğ’—_ğ’‹}$
            - Initializing unit random vector and store it in $ğ‘½$
            - Updating with $ğœ½$ for each learning iteration
    - Discussions
        - Better generalization performance
            - Learning feature presentation and the corresponding metric, not a specific class
            - Eliminating the need for computing and storing the gradients for {$ğ’˜_ğ’‹$}
                - Making it more scalable for big data aplications
      
#### Noise-contrastive estimation  
  - Prohibitive cost with non-parametric softmax in the situation where n is very large
      - Using Noise-contrastive estimation to approximate the full softmax
  - Casting the multi-class classification into the binary classification
      - Discriminating between data samples and noise samples
  - Formalizing the noise distribution as a uniform distribution: $ğ‘·_ğ’=ğŸ/ğ’$
  - Assuming that noise samples are $ğ’$ times more frequent than data samples
  - $ğ’‰(ğ’Š,ğ’—)â‰”ğ‘·(ğ‘«=ğŸâ”‚ğ’Š,ğ’—)=(ğ‘·(ğ’Šâ”‚ğ’—))/(ğ‘·(ğ’Šâ”‚ğ’—)+ğ’ğ‘·_ğ’ (ğ’Š) $
      - Posterior probability of sample ğ’Š with feature ğ’— being from the data distribution  
      - Correction of the probability that a ğ’Š and ğ’— are from the data distribution to the probability that they are from the noise distribution  
      - Training objective is to minimize the negative log-posterior distribution of data and noise sample.  
      - $ğ‘±_ğ‘µğ‘ªğ‘¬ (ğœ½)=âˆ’ğ‘¬_(ğ‘·_ğ’… ) [ğ¥ğ¨ğ ğ’‰(ğ’Š, ğ’—)]âˆ’ğ’Â·ã€–ğ‘¬_ğ‘·ã€—_ğ’ [ğ¥ğ¨ğ â¡(ğŸ âˆ’ ğ’‰(ğ’Š, ğ’— â€²))]$
          - $ğ‘·_ğ’…$: actual data distribution  
          - $ğ‘·_ğ’$: noise distribution 
          - $ğ’—â€²$: feature from randomly sampled according to $ğ‘·_ğ’$

#### Proximal Regularization  
  - Only having one instance per class unlike typical classification settings where each class has many instances
      - Oscillating learning process with lack of learning or long time to convergence
  - Solving by the proximal regularization method
      - Introducing an additional term to encourage the smoothness of the training dynamics
    ![image](https://github.com/MINJEONG-L/NPID/assets/82145878/e7f341c2-215b-4339-8e49-eca8bf073944)

  - $ğ‘±_ğ‘µğ‘ªğ‘¬ (ğœ½)=âˆ’ğ‘¬_(ğ‘·_ğ’… ) [ğ¥ğ¨ğ ğ’‰(ğ’Š,ğ’—_ğ’Š^((ğ’•âˆ’ğŸ) ) )âˆ’ğ€â€–ğ’—_ğ’Š^((ğ’•) )âˆ’ğ’—_ğ’Š^((ğ’•âˆ’ğŸ) ) â€–_ğŸ^ğŸ ]âˆ’ğ’ â‹…ã€–ğ‘¬_ğ‘·ã€—_ğ’ [ğ¥ğ¨ğ (ğŸâˆ’ğ’‰(ğ’Š,ğ’—^â€²(ğ’•âˆ’ğŸ)  ))]$
      - ğ’•: current iteration  
      - ğ€: proximal regularization impact degree
   
#### Weighted k-Nearest Neighbor Classifier 
  - Using Weighted K-Nearest Neighbor Classifier as model for differentiating test images
  - Process for classifying test image ğ’™Â Ì‚
      1. Computing ğ’‡Â Ì‚=ğ’‡_ğœ½ (ğ’™Â Ì‚) and comparing it against the embeddings in the memory bank using the cosine similarity ğ’”_ğ’Š  
        - ğ’”_ğ’Š=ğ’„ğ’ğ’”â¡(ğ’—_ğ’Š,ğ’‡ Ì‚)  
      2. Selecting the top ğ’Œ nearest neighbors denoted by ğ‘µ_ğ’Œ  
        - Making the prediction via weighted voting  
      3. Computing the contributing weight of neighbor ğ’™_ğ’Š  
        - ğœ¶_ğ’Š=ğ’†ğ’™ğ’‘â¡(ğ’”_ğ’Š/ğ‰)  
      4. Getting a total weight ğ’˜_ğ’„, classifying as the highest value class  
        - ğ’˜_ğ’„=âˆ‘_ğ’‹â–’ğœ¶_ğ’Š â‹…ğŸ(ğ’„_ğ’Š=ğ’„)
### Experiments  
#### Parametric vs. Non-parametric Softmax  
  - Parametric softmax
      - Obtaining accuracy of 60.3% and 63.0% with linear SVM* and kNN* classifiers respectively  
  - Non-parametric softmax
      - Rising accuracy to 75.4% and 80.8% for the linear and nearest neighbor classifiers  
  - NCE* approximating non-parametric softmax
       - Controlling the approximation using m  
       - When m = 4,096, the accuracy approaches that at m = 49,999 â€“ full form evaluation without any approximation
           - Providing assurance that NCE is an efficient approximation.  
    ![image](https://github.com/MINJEONG-L/NPID/assets/82145878/88be13c2-ff1f-446b-a9a6-728d38a39d50)  
    
